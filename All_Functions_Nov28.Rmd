---
output:
  reprex::reprex_document:
    venue: "gh"
    advertise: FALSE
    session_info: TRUE
    style: TRUE
    comment: "#;-)"
    tidyverse_quiet: FALSE
    std_out_err: TRUE
knit: reprex::reprex_render
---

```{r}
library("pdfetch") 
library("xts") 
library("reshape2") 
library("ggplot2") 
library("ggthemes") 
library("splines")
library("mgcv")
library("YieldCurve")
library("tidyverse")
library("dplyr")
library("splines2")
library("nlme")
library("tidyverse")
library("tidymodels")
library("MASS")
library("tidyr")
library("plotly")
```

```{r}
clean_yield_raw <- read.csv('Clean_Yields.csv')
maturity_clean <- as.vector(c(1/12, 1/4, 1/2, 1, 2, 3, 5, 7, 10, 20, 30))
clean_yield_raw <- clean_yield_raw
clean_yield_raw <- as.matrix(clean_yield_raw)
full_data <- list()

for(i in 1:nrow(clean_yield_raw)){
  one_date_matrix <- tibble(
    Maturity = maturity_clean,
    ZERO_YLD1 = as.numeric(clean_yield_raw[i,2:12 ]),
    START_DT = format(as.Date(clean_yield_raw[i,1], format="%Y-%m-%d"), "%Y%m%d")
  )
  one_date_matrix <- na.omit(one_date_matrix)

  full_data[[i]] <- one_date_matrix
}

full_data[[1]]

```

## tidying data
```{r}
# Load the dataset
data <- read.csv('spotSample.csv')

# Format of start date and end date
data$Maturity <- round((as.numeric(as.Date(as.character(data$MATUR_DATE), format="%Y%m%d") - 
                            as.Date(as.character(data$START_DT), format="%Y%m%d")) / 365)*12, 1/12) /12
# Arrange date by order of time to maturity
data <- data %>% 
mutate(Maturity = round(as.numeric(as.Date(as.character(MATUR_DATE), format="%Y%m%d") - 
as.Date(as.character(START_DT), format="%Y%m%d")) / 365*12, 1/12)/12) %>%
arrange(Maturity) 

# Plot the data by start date, which ends up with a nested list
date_list <- data %>%
  group_by(START_DT) %>%
  group_split() %>%
  lapply(function(sub_tibble) {
    sub_tibble %>% distinct() %>% arrange(Maturity) 
  }) 
  
standard_mats <- date_list[[1]]$Maturity
std_int_knots <- as.vector(c(1/12, quantile(standard_mats, probs= c(.2,.4,.6,.8)), 30))
```


Compute the Basis function using the Cox-deBoor Algorithm.
This function was copied from the CRAN document
https://cran.r-project.org/web/packages/crs/vignettes/spline_primer.pdf
I did my own implementation of this function but it is not as nice.

This is just an implementation of the Cox-deBoor recursion

```{r}
basis <- function(x, degree, i, knots){
  if(degree == 0){
    B <- ifelse((x>=knots[i])&(x<knots[i+1]),1,0)
  } else {
    if((knots[degree + i] - knots[i]) == 0){
      if(x != knots[i+degree]){
        alpha1 <- 0
      } else {
        return(1)
      }
    } else {
      alpha1 <- (x-knots[i])/(knots[degree+i] - knots[i])
    }
    if((knots[i+degree+1] - knots[i+1]) == 0){
      if(x != knots[i+degree]){
        alpha2 <- 0
      } else {
        return(1)
      }
    } else {
      alpha2 <- (knots[i+degree+1] - x) / (knots[i+degree+1] - knots[i+1])
    }
    B <- alpha1 * basis(x, (degree-1), i, knots) + 
      alpha2*basis(x, (degree-1), (i+1), knots)
  }
  return(B)
}

```

Construct the matrix $\mathbf{B}$, column by column.
Matrix $B$ contains the basis functions computed through the Cox-deBoor recursive formulas using the function defined in the codechunk above, evaluated at different times to maturity (rows) and at different knot intervals (columns). For more details, check FURM on canvas

```{r}
matrix_b <- function(x, degree=3, int_knots) { 
  # the x argument takes in a vector of time values that 
  # will be used to evaluate a design matrix of basis functions 
  # the degree argument specifies the highest degree of polynomials for
  # the basis functions
  # the int_knots argument takes in a vector of knots that will be used 
  # to determine the intervals of the piecewise function
  bound_knots <- int_knots[c(1, length(int_knots))] # this line creates bound knots
  knots <- c(rep(bound_knots[1], (degree+1)), int_knots[c(-1, -length(int_knots))], rep(bound_knots[2], (degree+1)))
  # the line above adds a couple of extra knots to each end of the int_knots vector because of the Cox-deBoor recursion
  K <- length(int_knots) + degree - 1 # number of columns in the Basis matrix
  B.mat <- matrix(0,nrow = length(x), ncol = K) # initialize the matrix
  for(j in 1:K) {
    B.mat[,j] <- sapply(X = x, FUN = basis, degree = degree, i = j, knots = knots) # add each column, one by one
  }
  return(B.mat) # return the matrix
}
```


test the functions on the yield rates provided by Avery
basically, minimize:

$$
\text{MSE}(\alpha|\mathbf{B}, r) = (r - \mathbf{B}\alpha)^T(r-\mathbf{B}\alpha)
$$

Using OLS (unpenalized)

$$
\alpha = (\mathbf{B}^T\mathbf{B})^{-1} \mathbf{B}^T r
$$
where $r$ is a vector of yield rates

Manny: I also changed this file a little, but the output is still a data frame with two columns
(yield, ttm) 
```{r}
interp_yc <- function(yield_list, int_knots, degree = 3, d, last_tenor){
  yield_list[[d]] <- data.frame(Maturity = yield_list[[d]]$Maturity,
                                ZERO_YLD1 = yield_list[[d]]$ZERO_YLD1)
  yc_df_pre <- rbind(data.frame(Maturity = 0, ZERO_YLD1 = 0), na.omit(yield_list[[d]]))
  last_row <- which(round(yc_df_pre$Maturity,3) == last_tenor)
  yc_df <- yc_df_pre[1:last_row,]
  yields <- c(0, yc_df$ZERO_YLD1)
  maturities <- c(0, as.numeric(yc_df$Maturity))
  x <- as.numeric(maturities) # maturity dates
  B <- matrix_b(x, degree=degree, int_knots = int_knots) 
  B_t_B <- t(B) %*% B
  # B is the design matrix on which the least squares coefficients will be calculated
  
  alphas <- solve(B_t_B) %*% t(B) %*% yields # OLS Formula for coefficients
  x2 <- seq(1/12, last_tenor, 1/12) # this range is used to simulate a continuous yield curve
  B2 <- matrix_b(x2, degree = degree, int_knots = int_knots) 
  # B2 is the matrix of basis functions but evaluated at a 'continuous' time (not really but close enough)
  
  interpolated_yields <- data.frame(Maturity = x2, ZERO_YLD1 = B2 %*% alphas) # create dataframes for plotting
  og_yields <- data.frame(ttm = maturities, yield = yields)

  return(interpolated_yields)
}
```

```{r}
interpolate_list <- function(yield_list, start, T_, degree = 3){
  interpolated_yc <- list()
  k <- 1
  for(i in start:(start + T_ - 1)){
    lt_max <- max(yield_list[[i]]$Maturity) # This line of code basically chops all yields beyond 20
    avail_ylds <- na.omit(yield_list[[i]]$ZERO_YLD1)
    maturities <- yield_list[[i]]$Maturity
    N <- length(avail_ylds)
    if(N %in% c(5, 6)){
      int_knots <- c(0, quantile(maturities, probs = c(0, 0.5, 1)))
    } else if(N %in% c(7,8,9)){
      int_knots <- c(0,quantile(maturities, probs = c(0, 0.33, 0.66, 1)))
    } else if(N %in% 10:15){
      int_knots <- c(0,quantile(maturities, probs = c(0, 0.25, 0.5, .75, 1)))
    } else {
      int_knots <- c(0,quantile(maturities, probs = c(0, 0.20, 0.4, .6, .8, 1)))
    }
    interpolated_yc[[k]] <- interp_yc(yield_list = yield_list,
                                      int_knots = int_knots,
                                      d = i,
                                      last_tenor = lt_max)[3:240,]
    k <- k + 1
  }
  return(interpolated_yc)
}
```

```{r}
# Couple of examples
int1 <- interpolate_list(full_data, 1000, 1)[[1]]
plot(full_data[[1000]]$Maturity, full_data[[1000]]$ZERO_YLD1)
lines(int1$Maturity, int1$ZERO_YLD1)

int2 <- interpolate_list(full_data, 6230, 1)[[1]]
plot(full_data[[6230]]$Maturity, full_data[[6230]]$ZERO_YLD1)
lines(int2$Maturity, int2$ZERO_YLD1)

```

This function is used for Synthetic data Simulation

```{r}
generate_data <- function(T_, betas = c(4, 0, -5), lambda = 0.5, GLS = FALSE, maturities = c(seq(1/12, 20, 10 / 3),20)) {
  test_data <- list() 
  N <- length(maturities)
  cov_matrix = NULL
  sigma2 = NULL
  # GLS setup with Wishart covariance matrix
  if (GLS) {
    df <- N
    sigma_matrix <- diag(N)
    cov_matrix <- rWishart(1, df = df, Sigma = sigma_matrix)[,,1] / 140
  } else {
    sigma2 <- runif(1, 0.01, 0.05)
    sigma <- sqrt(sigma2)
  }
  

  for (j in 1:T_) {
    # Fixed betas for simplicity
      # Random epsilon

    # Vectorized computation for ZERO_YLD1
    term1 <- betas[1]
    term2 <- betas[2] * ((1 - exp(-lambda * maturities)) / (lambda * maturities))
    term3 <- betas[3] * (((1 - exp(-lambda * maturities)) / (lambda * maturities)) - exp(-lambda * maturities))

    if (GLS) {
      noise <- mvrnorm(1, mu = rep(0, N), Sigma = cov_matrix)
    } else {
      noise <- rnorm(N, mean = 0, sd = sigma)
    }

    ZERO_YLD1 <- term1 + term2 + term3 + noise

    # Create dataset for current time point
    data <- tibble(Maturity = maturities, ZERO_YLD1 = ZERO_YLD1)
    test_data[[j]] <- data

  }

  return(list(test_data = test_data, betas = betas, cov_mat = cov_matrix, sigma2 = sigma2, lambda = lambda))
}

```

This is the code for NS over a time window, uses the algorithm that zimo explained to us (Static OLS form)
```{r}
# Fit and plot the fitted nelson siegel model
# over a time window
fit_nelson_siegel <- function(yield_list, lambda, start, tenors, T_){
  # yield_list: Parameter of the form of a list of data frames containing ZCB yields
  # int knots: The interior knots used for b-spline construction
  # lambda: Individual lambda parameter for NS
  # row_idx: The row indices used for decimating the yield curve after bootstrap and interpolation
  # start: starting date from the yield_list list
  # T_: length of time window
  indices <- which(round(yield_list[[start]]$Maturity, 3) %in% round(tenors, 3))
  maturities <- yield_list[[1]]$Maturity[indices]
  N <- length(maturities)
  
  term1 <- 1
  term2 <- (1 - exp(-maturities*lambda)) / (maturities*lambda)
  term3 <- ((1 - exp(-maturities*lambda)) / (maturities*lambda)) - exp(-maturities*lambda)
  Phi <- cbind(term1, term2, term3) # Construct Phi matrix for NS
  
  Y_mat <- matrix(0, nrow = N, # matrix of N by T, containing yields for each tenor (columns)
                  ncol = T_) # where each column represents a different date
  j <- 1
  for(t in start:(start + T_- 1)){
    Y_mat[,j] <- yield_list[[t]]$ZERO_YLD1[indices]
    j <- j + 1
    phitphi_1phit <- solve(t(Phi) %*% Phi) %*% t(Phi) # OLS for the coefficients for every single day
    betas_t <- phitphi_1phit %*% Y_mat  
    betas <- rowSums(betas_t) / T_ # average all coefficients
    eps <- matrix(0, nrow = N, ncol = T_) # matrix of errors for each day (column) and each tenor (row)
    for(t in 1:T_){
      eps[,t] <- Y_mat[,t] - Phi %*% betas # Populate errors
    }
    sig_hat2 <- sum(as.vector(eps)^2) / (N * T_ - 3) # take mean squared error (MLE Estimator)
  }

  return(list(betas = betas, # fitted betas static: 1*3   dynamic: T*3
              sigma2 = sig_hat2, # MSE
              lambda = lambda, # lambda(input)
              Phi = Phi, # Nelson Siegel design matrix N*3
              eps = eps)) # residuals N*T
  
}
```

Code for static GLS

```{r}
fit_nelson_siegel_GLS <- function(yield_list, lambda, start, T_, tenors) {
  # The tenors parameter takes into account the case where we're setting yield_list
  # to be an interpolated yield curve with many data points
  indices <- which(round(yield_list[[start]]$Maturity, 3) %in% round(tenors, 3))

  max_iteration <- 1000 # max iteration
  maturities <- yield_list[[1]]$Maturity[indices]
  N <- length(maturities) # number of tenor
  term1 <- 1
  term2 <- (1 - exp(-maturities * lambda)) / (maturities * lambda)
  term3 <- ((1 - exp(-maturities * lambda)) / (maturities * lambda)) - exp(-maturities * lambda)
  Phi <- cbind(term1, term2, term3)
  
  # Initialize the matrix for observed yields (Y_mat)
  Y_mat <- matrix(0, nrow = N, ncol = T_)
  
  # Populate Y_mat with yields for each tenor
  j <- 1
  for(t in start:(start + T_ - 1)) {
    Y_mat[,j] <- yield_list[[t]]$ZERO_YLD1[indices]
    j <- j + 1
  }

  avg_cov <- diag(N)
  betas <- c(0, 0, 0) # initial beta

  # Iterative GLS estimation
  for (i in 1:max_iteration) {
    # Transform Y and Phi for the GLS step
    L <- t(chol(avg_cov))
    Y_mat_trans <- solve(L, Y_mat)
    Phi_trans <- solve(L, Phi)
    
    # Compute new betas by OLS case
    phitphi_inv <- solve(t(Phi_trans) %*% Phi_trans)
    betas_new <- rowMeans(phitphi_inv %*% t(Phi_trans) %*% Y_mat_trans)
    # Check for convergence
    if (sum((betas - betas_new)^2) < 1e-8) {
      betas <- betas_new
      break
    }
    # Update betas
    betas <- betas_new
    fitted_yields <- Phi %*% betas  # compute the estimated beta
    fitted_yields_matrix <- matrix(fitted_yields, nrow = N, ncol = T_, byrow = FALSE)
    
    # Compute residuals
    eps <- Y_mat - fitted_yields_matrix
    avg_cov <- eps %*% t(eps) / (T_ - 3)
  }
  
  # Final sigma estimate based on residuals
  sig_hat <- sum(eps^2) / (N * T_ - 3)
  return(list(betas = betas, cov_mat = avg_cov, lambda = lambda, Phi = Phi, eps = eps))
}
```


Simulations for OLS fitting tests:

```{r}
# Generate Data

OLS_syn_data <- generate_data(100, maturities = c(1/12,2,4,6,18,20))
OLS_syn_yields <- OLS_syn_data$test_data
OLS_syn_betas <- OLS_syn_data$betas
OLS_syn_sigma2 <- OLS_syn_data$sigma

OLS_interp_yields <- interpolate_list(OLS_syn_yields,
                 1,
                 100,
                 3)

plot(OLS_syn_yields[[1]]$Maturity, OLS_syn_yields[[1]]$ZERO_YLD1)
lines(OLS_interp_yields[[1]]$Maturity, OLS_interp_yields[[1]]$ZERO_YLD1)
OLS_syn_fit <- fit_nelson_siegel(OLS_interp_yields,
                                 lambda = 0.5,
                                 start = 1,
                                 T_ = 100,
                                 tenors = c(1/4,5,10,15,20))

# Check for sigma estimation
OLS_syn_sigma2
OLS_syn_fit$sigma2 

# Check for beta estimation
OLS_syn_betas
as.vector(OLS_syn_fit$betas)
nrow(OLS_syn_fit$eps)
```

Simulations for GLS fitting tests:

```{r}
# Generate Data

GLS_syn_data <- generate_data(1000, GLS = T, maturities = c(1/12,2,4,6,8,10,12,14,16,18,20))
GLS_syn_yields <- GLS_syn_data$test_data
GLS_syn_betas <- GLS_syn_data$betas
GLS_syn_cov_mat <- GLS_syn_data$cov_mat

# Interpolate

GLS_interp_yields <- interpolate_list(GLS_syn_yields,
                                      1, 100, 3)
plot(GLS_syn_yields[[10]]$Maturity, 
     GLS_syn_yields[[10]]$ZERO_YLD1)
lines(GLS_interp_yields[[10]]$Maturity,
     GLS_interp_yields[[10]]$ZERO_YLD1)

GLS_syn_fit <- fit_nelson_siegel_GLS(GLS_interp_yields,
                                 lambda = 0.5,
                                 start = 1,
                                 T_ = 50,
                                 tenors = c(1/4,3,6,12,15,20))
# Check for covariance estimation

GLS_syn_cov_mat
GLS_syn_fit$cov_mat

# Check for beta estimation
GLS_syn_betas
as.vector(GLS_syn_fit$betas)

```

The following function can be used to calculate profile likelihood of $\lambda$ parameter

```{r}
get_likelihood <- function(yield_list, lambda_list, start = 1, T_ = 5, GLS = T, tenors) {
  log_likelihoods <- numeric(length(lambda_list))
  indices <- which(round(yield_list[[start]]$Maturity, 3) %in% round(tenors, 3))

  N <- length(yield_list[[start]]$Maturity[indices])
  
  if(GLS == F){
    for(i in seq_along(lambda_list)) {
      lambda <- lambda_list[i] 
      fit_model <- fit_nelson_siegel(yield_list, 
                                     lambda = lambda, 
                                     start = start, T_ = T_, tenors = tenors) # estimate the parameters for given lambda
      betas <- fit_model$betas 
      sigma_hat2 <- fit_model$sigma2
      Phi <- fit_model$Phi
      e <- fit_model$eps
      log_likelihoods[i] <- -T_ / 2 * (N * log(2*pi) + N * log(sigma_hat2)) - (N * T_ - 3) / 2 # The expression in the exponential simplifies to this in the OLS case
    }
    df_likelihood <- data.frame(lambda = lambda_list,
                                log_likelihood = log_likelihoods)
    lk_plot <- ggplot() +
      geom_line(data = df_likelihood, aes(x = lambda, y = log_likelihood)) +
      geom_vline(xintercept = lambda_list[which(log_likelihoods == max(log_likelihoods))],
                 color = "red", linetype = "dashed")
    
  } else {
    for(i in seq_along(lambda_list)) {
      lambda <- lambda_list[i] 
      fit_model <- fit_nelson_siegel_GLS(yield_list = yield_list, 
                                         lambda = lambda, 
                                         start = start, T_ = T_, 
                                         tenors = tenors) # fit model
      
      betas <- fit_model$betas
      cov_mat <- fit_model$cov_mat
      Phi <- fit_model$Phi
      e <- fit_model$eps
      comp_1 <- -T_*(N * log(2*pi) + log(det(cov_mat)))/2
      # split the calculation into two components
      sum_comp_2 <- 0
      for(t in 1:T_){
        sum_comp_2 <- sum_comp_2 + t(e[,t]) %*% solve(cov_mat) %*% e[,t] # iterate through dates
      }
      
      comp_2 <- -1/2*sum_comp_2 
      
      log_likelihoods[i] <- comp_1 + comp_2 
    }
    df_likelihood <- data.frame(lambda = lambda_list,
                                log_likelihood = log_likelihoods)
    lk_plot <- ggplot() + 
      geom_line(data = df_likelihood, aes(x = lambda, y = log_likelihood)) +
      geom_vline(xintercept = lambda_list[which(log_likelihoods == max(log_likelihoods))],
                 color = "red", linetype = "dashed")
  }
  
  return(list(lk_plot = lk_plot, log_likelihoods = log_likelihoods, max_log_likelihood = max(log_likelihoods),lambda_grid = lambda_list, lambda = lambda_list[which(log_likelihoods == max(log_likelihoods))]))
}
```

Use this chunk to test the likelihood function. To run a larger test I would recommend to just do another file.
```{r}
Prof_Lik_Data <- generate_data(10, GLS = F, maturities = c(1/12,2,4,6,8,10,12,14,16,18,20), betas = c(4,2,3), lambda = 0.2)
Prof_Lik_yields <- Prof_Lik_Data$test_data
Prof_Lik_betas <- Prof_Lik_Data$betas

interp_Lik_yields <- interpolate_list(Prof_Lik_yields, 1, 10)
plot(Prof_Lik_yields[[1]])
lines(interp_Lik_yields[[1]])

get_likelihood(yield_list = interp_Lik_yields, 
               lambda_list = seq(0.1, .8, 0.05), 
               start = 4, 
               T_ = 1, 
               GLS = F, 
               tenors = c(1/4,5,10,15,20))$log_likelihoods
```

Try playing around with it by using different tenors for NS parameter fitting.
Use interpolated_list function to fitting using tenors not found in original (synthetic) data
```{r}
fit_NS_parameters <- function(yield_list, lambda_list = seq(0.1, 0.6, 0.01), GLS = FALSE, static = TRUE, start, T_, tenors = c(1/12, 3, 6, 9, 12, 16, 20)) {
  fit_obj <- list() 
  log_liks_list <- get_likelihood(yield_list = yield_list,
                                                  lambda_list = lambda_list,
                                                  start = start,
                                                  T_ = T_,
                                                  GLS = GLS,
                                                  tenors = tenors)

  log_liks <- log_liks_list$log_likelihoods
  log_liks_plot <- log_liks_list$lk_plot
  best_lambda <- log_liks_list$lambda
  max_log_lik <- log_liks_list$max_log_likelihood
  
  print(max_log_lik)
  if(GLS){
    print(best_lambda)
    best_fit <- fit_nelson_siegel_GLS(yield_list = yield_list,
                                      lambda = best_lambda,
                                      start = start,
                                      T_ = T_,
                                      tenors = tenors)
    return(list(lambda = best_lambda, log_likelihood = max_log_lik, betas = best_fit$betas))
  } else {
    best_fit <- fit_nelson_siegel(yield_list = yield_list,
                                  lambda = best_lambda,
                                  start = start,
                                  T_ = T_,
                                  tenors = tenors)
    return(list(lambda = best_lambda, log_likelihood = max_log_lik, betas = best_fit$betas, sigma2 = best_fit$sigma2))
  }
}
```


```{r}
fit_NS_GLS_syn <- generate_data(100, GLS = T, lambda = 0.25)

fit_NS_GLS_yields <- fit_NS_GLS_syn$test_data

interp_NS_GLS <- interpolate_list(fit_NS_GLS_yields,
                                  1,100,3)

plot(fit_NS_GLS_yields[[5]]$Maturity, fit_NS_GLS_yields[[1]]$ZERO_YLD1)
lines(interp_NS_GLS[[5]]$Maturity, interp_NS_GLS[[1]]$ZERO_YLD1)

fit_NS_parameters(interp_NS_GLS, start = 1, T_ = 100, tenors = c(1/4, 5,10,15,20), GLS = T)
```

```{r}
fit_NS_OLS_syn <- generate_data(300, GLS = F, lambda=0.2, betas = c(4,2,-3))

fit_NS_OLS_yields <- fit_NS_OLS_syn$test_data

interp_NS_OLS <- interpolate_list(fit_NS_OLS_yields,
                                  1,300,3)
plot(fit_NS_OLS_yields[[1]])
lines(interp_NS_OLS[[1]])
fit_NS_parameters(interp_NS_OLS, start = 1, T_ = 300, tenors = c(1/4,5,10,15,20))
```

```{r}
compute_yield <- function(tenors, betas,lambda) { # show no points
  term1 <- betas[1]
  term2 <- betas[2] * ((1 - exp(-lambda * tenors)) / (lambda * tenors))
  term3 <- betas[3] * (((1 - exp(-lambda * tenors)) / (lambda * tenors)) - exp(-lambda * tenors))
  yield <- term1 + term2 + term3
  return(yield)
}

plot_fitted_NS <- function(yield_list, lambda = 0.5, start = 1, T_ = 20, tenors) {
  OLS_simulated_fit <- fit_nelson_siegel(yield_list, lambda = 0.5, start, T_ = 20, tenors= tenors)
  computed_yields <- data.frame(Maturity = seq(1/12,20,length.out=100),
                              ZERO_YLD1 = compute_yield(seq(1/12,20,length.out=100),
                                                          OLS_simulated_fit$betas, 0.5))
  computed_yields$source <- 'fitted'
  ggplot() + geom_line(data = computed_yields, aes(x = Maturity, y = ZERO_YLD1, color = source))
  
}
```

