---
title: "All Functions"
format: pdf
---
```{r}
library(tibble)
library(MASS)
``` 



# KF_Prediction (one step ahead prediction)
```{r}
#This  funciton is used for one step ahed prediction

FK_prediction <- function(A, B, C, D, R, Q, last_x, last_x_var){
  # A: state transition matrix for latent paramters x (3 * 3)
  # B: process noise coefficient matrix (3 * m ;  default: 3 * 3 diagonal)
  # C: observation matrix (N * 3 where N is the number of tenors in a day)
  # D: time-varying measurement noise coefficient matrix (N * k;  detaul: N * N, where N is the number of tenors in a day)
  # Q: process noise covariance matrix (m * m ; default: 3 * 3 ) 
  # R: measurement noise covariance matrix (k * k, dedault N * N)
  # last_x: x_{t-1|t-1}, the expectation of latent paramter in the last state
  # last_x_var: Sigma_{t-1|t-1}, the covariance matrix in the last state
  
  E_x_t <- A %*% last_x  # x_{t|t-1} one step ahead prediction for latent paramter
  Var_x_t <- A %*% last_x_var %*% t(A)  + B %*% Q  %*%  B # Sigma_{t|t-1} covariance matrix of one step ahead prediction for latent paramter
  
  E_y_t <- C %*% A %*% last_x # y_{t|t-1} one step ahead prediction for reponse variable
  Var_y_t <- C %*% (A %*% last_x_var %*% t(A) + B %*% Q %*% t(B)) %*% t(C) + D %*% R %*% t(D) # F, covariance for y prediction
  
  
  return(list(
    E_x_t = E_x_t,
    Var_x_t = Var_x_t,
    E_y_t = E_y_t,
    Var_y_t = Var_y_t
  ))
}
```
# get_C (compute C matrix)
```{r}
get_C <- function(lambda, tenors = c(1/12,3/12,6/12,1,2,3,5,7,10,20)) {
  
  # Compute basis functions
  B0 <- rep(1, length(tenors))
  B1 <- (1 - exp(-lambda * tenors)) / (lambda * tenors)
  B2 <- B1 - exp(-lambda * tenors)
  C_matrix <- cbind(B0, B1, B2)
  
  return(C_matrix)
}

T_ <- 960 
```




# Partial Log-Likelihood Approximation
```{r}
# Partial Likelihood Mid-point Approximation
KF_likelihood <-function(A, B, C, D, Q, R, last_Sig, y_t, cur_x){
  
  F_t <- C %*% (A %*% last_Sig %*% t(A) + B %*% Q %*% t(B)) %*% t(C) + D %*% R %*% t(D)
  
  e_t <- as.matrix(y_t - C %*% cur_x)
  # print(cat("a",as.numeric(t(e_t) %*% ginv(F_t, 1e-4) %*% e_t)))
  log_likelihood_t <- -0.5 * (log(det(F_t)) + t(e_t) %*% solve(F_t) %*% e_t )
  
  return(log_likelihood_t)
} 


#get_partial_R(last_x, last_Sig, cur_x, cur_Sig, last_partial_x_R, last_partial_Sig_R, F_t, e_t, A, B, C, D, Q, R)
partial_A_approx <- function(A, B, C, D, Q, R, last_Sig, y_t, cur_x, h=1e-3){
  log_likelihood_0 <- KF_likelihood(A, B, C, D, Q, R, last_Sig, y_t, cur_x)
  grad_A <- matrix(0, nrow=nrow(A), ncol=ncol(A))
  
  # for (i in 1:nrow(A)) {
  #   for (j in 1:ncol(A)) {
  #     # Create perturbed matrices
  #     A_plus <- A
  #     A_minus <- A
  #     
  #     A_plus[i, j] <- A_plus[i, j] + h
  #     A_minus[i, j] <- A_minus[i, j] - h
  # 
  #     log_likelihood_plus <- KF_likelihood(A_plus, B, C, D, Q, R, last_Sig, y_t, cur_x)
  #     log_likelihood_minus <- KF_likelihood(A_minus, B, C, D, Q, R, last_Sig, y_t, cur_x)
  #     
  #     grad_A[i, j] <- (log_likelihood_plus - log_likelihood_minus) / (2 * h)
  #   }
  # }
  
  
  return(list(grad_A = grad_A, log_likelihood_0 = log_likelihood_0))
}
```


```{r}
partial_Q_approx <- function(A, B, C, D, Q, R, last_Sig, y_t, cur_x, h=1e-5){
  log_likelihood_0 <- KF_likelihood(A, B, C, D, Q, R, last_Sig, y_t, cur_x)
  
  grad_Q <- matrix(0, nrow=nrow(Q), ncol=ncol(Q))
  
  for (i in 1:nrow(Q)) {
    for (j in 1:ncol(Q)) {
      # Create perturbed matrices
      Q_plus <- Q
      Q_minus <- Q
      
      Q_plus[i, j] <- Q_plus[i, j] + h
      Q_minus[i, j] <- Q_minus[i, j] - h

      log_likelihood_plus <- KF_likelihood(A, B, C, D, Q_plus, R, last_Sig, y_t, cur_x)
      log_likelihood_minus <- KF_likelihood(A, B, C, D, Q_minus, R, last_Sig, y_t, cur_x)

      grad_Q[i, j] <- (log_likelihood_plus - log_likelihood_minus) / (2 * h)
    }
  }
  return(grad_Q)
}





partial_R_approx <- function(A, B, C, D, Q, R, last_Sig, y_t, cur_x, h=1e-5){
  log_likelihood_0 <- KF_likelihood(A, B, C, D, Q, R, last_Sig, y_t, cur_x)
  
  grad_R <- matrix(0, nrow=nrow(R), ncol=ncol(R))
  
  for (i in 1:nrow(R)) {
    for (j in 1:ncol(R)) {
      # Create perturbed matrices
      R_plus <- R
      R_minus <- R
      
      R_plus[i, j] <- R_plus[i, j] + h
      R_minus[i, j] <- R_minus[i, j] - h

      log_likelihood_plus <- KF_likelihood(A, B, C, D, Q, R_plus, last_Sig, y_t, cur_x)
      log_likelihood_minus <- KF_likelihood(A, B, C, D, Q, R_minus, last_Sig, y_t, cur_x)
      
      # print(c(log_likelihood_plus,log_likelihood_minus))

      grad_R[i, j] <- (log_likelihood_plus - log_likelihood_minus) / (2 * h)
    }
  }
  return(grad_R)
}





```



# case studies (not finished)
```{r}
set.seed(123)
beta <- c(5, -3, 3)

A_stable <- matrix(c(0.1, 0, 0,
                  0, 0.1, 0,
                  0, 0, 0.1), 
            nrow = 3, byrow = TRUE)
A_stable
mat<- A_stable
for( i in 1:200){
  mat <- mat %*% A_stable
}
mat

T_ <- 30

beta_values <- matrix(0, nrow = T_, ncol = 3)

beta_values[1, ] <- beta

for (t in 2:T_) {
  beta_values[t, ] <- A_stable %*% beta_values[t - 1, ]
}

data <- list()
for( i in 1:T_){
  data[[i]] <- generate_data(T_ = 1, betas = beta_values[i, ], lambda = 0.5, GLS = F)$yield_list[[1]]
}

data[[1]]
data[[1]][1]






# partial_log_A_t <- partial_A_approx(A, B, C, D, Q, R, last_Sig = last_Sig, y_t = synthetic_OLS_yields_Long_R[[3]][2], cur_x = cur_x, h=1e-5)
# partial_log_Q_t <- partial_Q_approx(A, B, C, D, Q, R, last_Sig = last_Sig, y_t = synthetic_OLS_yields_Long_R[[3]][2], cur_x = cur_x, h=1e-5)
# partial_log_R_t <- partial_R_approx(A, B, C, D, Q, R, last_Sig = last_Sig, y_t = synthetic_OLS_yields_Long_R[[3]][2], cur_x = cur_x, h=1e-5)
```



```{r}

KF_Estimate_approx <- function(yields, T_, tenors = c(1/12,3/12,6/12,1,2,3,5,7,10,20), lambda = 0.5){
    # yield: Parameter of the form of a list of data frames containing ZCB spot rate
    # T_: length of time window
    # tenors: list of time to maturities
    # lambda_list: grid of lambda
  
    N <- length(tenors) # numerb of tenors
    
    # Initialization of parameters 
    # In the early stage, we assume default dimension for Q and R, B hence D to be 3*3/n*n identity matrix 
    A <- diag(3) # 3 * 3
    B <-diag(3) # 3 * 3
    C <- get_C(lambda, tenors = tenors) #  nelson siegel design matrix
    D <- diag(N) # N * N
    Q <- 0.1 * diag(3) # 3 * 3
    R <- 0.1 * diag(N) # N * N
    
     # place holder for last result iteration of parameter estimation
    lastA <- 10*diag(3)  
    lastQ <- 10*diag(3)
    lastR <- 10*diag(N) 
    
    # Initialization of partial derivatives w.r.t paramters
    partial_log_l_A <- diag(3)
    partial_log_l_Q <- diag(3)
    partial_log_l_R <- diag(N)
    num_run <- 1
    
    y_t <- vector("list", T_)  
    for (t in 1:T_) {
      y_t[[t]] <- as.matrix(yields[[t]][2])  
    }
    
    repeat{
        
        # Initilaiation of paramters
        
        last_x <- matrix( 1, ncol =1, nrow = 3) # x_{0|0}
        last_Sig <- diag(3) #Sigma_{0|0}
        
        # iterate along time 1:T_
        for(i in 1:length(yields)){  
          
            pred_res <- FK_prediction(A, B, C, D, R, Q, last_x, last_Sig) # one step ahead prediction of x and y
            
            cur_x <- pred_res$E_x_t
            cur_Sig <- pred_res$Var_x_t
            cur_y <- pred_res$E_y_t
            F_t <- pred_res$Var_y_t
            
            K_t <- cur_Sig %*% t(C) %*% ginv(F_t) # Kalman Gain 3 * n
            e_t <- y_t[[i]] - C %*% cur_x # innovation N * 1
            
            # state estimate update
            next_x <- cur_x + K_t %*% (y_t[[i]] - C %*% cur_x) # x_{t|t}
            next_Sig <- cur_Sig - K_t %*% C %*% cur_Sig # Sigma_{t|t}
             
            
            # calcualte the partial derivatives w.r.t. each paramter
            partial_log_A_t <- partial_A_approx(A, B, C, D, Q, R, last_Sig = last_Sig, y_t = y_t[[i]], cur_x = cur_x, h=1e-5)
            partial_log_Q_t <- partial_Q_approx(A, B, C, D, Q, R, last_Sig = last_Sig, y_t = y_t[[i]], cur_x = cur_x, h=1e-5)
            partial_log_R_t <- partial_R_approx(A, B, C, D, Q, R, last_Sig = last_Sig, y_t = y_t[[i]], cur_x = cur_x, h=1e-5)
           
            # add the current time partial_log_likelihood to the summation of partial_log_likelihood over time 1:T_
            partial_log_l_A <- partial_log_l_A + partial_log_A_t$grad_A
            partial_log_l_Q <- partial_log_l_Q + partial_log_Q_t
            partial_log_l_R <- partial_log_l_R + partial_log_R_t
            last_x <- next_x # x_{t|t}
            last_Sig <- next_Sig  # Sigma_{t|t}
            
        
        }
        num_run <- num_run +1
        
        alpha <- 0.000001
        # update the 
 
        A <- A + alpha * partial_log_l_A
        Q <- Q + alpha * partial_log_l_Q
          
        R <- R + alpha * partial_log_l_R
        
        
        # Compute the convergece condition by the ratio of difference of paramters, using Euclidean norm
        # num <- norm(A - lastA, type = "F") + norm(Q - lastQ, type = "F") + norm(R - lastR, type = "F") # 
        # denom <- norm(A, type = "F") + norm(Q, type = "F") + norm(R, type = "F")
        # ratio <- num/denom
        ratio_A <- norm(A - lastA, type = "F")/norm(A, type = "F")
        ratio_Q <- norm(Q - lastQ, type = "F")/norm(Q, type = "F")
        ratio_R <- norm(R - lastR, type = "F")/norm(R, type = "F")
        ratio <- max(c(ratio_A, ratio_Q, ratio_R))
        # print(partial_log_l_A)
        print(ratio)
        
        if(ratio < 0.01){
          break
        }
        
        # parameter update for the next iteration
        
        lastA <- A
        lastQ <- Q
        lastR <- R
        
        
        
    }
      
    return(list(A = A, B = B, C = C, D = D, R = R, Q = Q))
  
}

KF_Estimate_approx(data, 30, tenors = c(1/12,3/12,6/12,1,2,3,5,7,10,20), lambda = 0.5)

#KF_Estimate_approx(data, 200, tenors = c(1/12,3/12,6/12,1,2,3,5,7,10,20), lambda = 0.5)
#KF_Estimate_approx(synthetic_OLS_yields_Long_R, 960, tenors = c(1/12,3/12,6/12,1,2,3,5,7,10,20), lambda = 0.5)
KF_Estimate_approx(synthetic_OLS_yields_Long_R, 960, tenors = c(1/12,3/12,6/12,1,2,3,5,7,10,20), lambda = 0.5)


mat <- matrix(c(
  2.256, 2.114, 2.056, 1.952, 1.787, 1.665, 1.502, 1.404, 1.318, 1.210,
  2.114, 2.176, 2.022, 1.925, 1.770, 1.654, 1.498, 1.402, 1.318, 1.210,
  2.056, 2.022, 2.074, 1.887, 1.746, 1.638, 1.491, 1.398, 1.316, 1.209,
  1.952, 1.925, 1.887, 1.917, 1.700, 1.607, 1.474, 1.389, 1.310, 1.206,
  1.787, 1.770, 1.746, 1.700, 1.716, 1.546, 1.438, 1.364, 1.294, 1.199,
  1.665, 1.654, 1.638, 1.607, 1.546, 1.591, 1.402, 1.338, 1.275, 1.189,
  1.502, 1.498, 1.491, 1.474, 1.438, 1.402, 1.438, 1.289, 1.240, 1.172,
  1.404, 1.402, 1.398, 1.389, 1.364, 1.338, 1.289, 1.352, 1.213, 1.158,
  1.318, 1.318, 1.316, 1.310, 1.294, 1.275, 1.240, 1.213, 1.284, 1.143,
  1.210, 1.210, 1.209, 1.206, 1.199, 1.189, 1.172, 1.158, 1.143, 1.222
), nrow = 10, byrow = TRUE)

log(det(mat))
```

```{r}
# Case study on the 
```

